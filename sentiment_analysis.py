# -*- coding: utf-8 -*-
"""Sentiment_Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10SRl4y5Mokx_KIC6GX4AXWzYYYTu6pXs
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string 
import nltk
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
# %matplotlib inline

train=pd.read_csv(r"/content/drive/My Drive/Project/train_E6oV3lV.csv")
test=pd.read_csv(r"/content/drive/My Drive/Project/test_tweets_anuFYb8.csv")

train.head()

combi=train.append(test,ignore_index=True)

def remove_pattern(input_txt,pattern):
  r=re.findall(pattern, input_txt)
  for i in r:
    input_txt= re.sub(i, "", input_txt)

    return input_txt

combi['tidy_tweet']=np.vectorize(remove_pattern)(combi['tweet'], "@[\w]*")

combi.head(10)

combi['tidy_tweet']=combi['tidy_tweet'].str.replace("[^a-zA-Z#]", " ")

combi.head(10)

combi['tidy_tweet']=combi['tidy_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))

combi.head(10)

tokenized_tweet=combi['tidy_tweet'].apply(lambda x: x.split())
tokenized_tweet.head(20)

from nltk.stem.porter import *
stemmer= PorterStemmer()

tokenized_tweet=tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])
tokenized_tweet.head(10)

all_words= " ".join([text for text in combi['tidy_tweet']])
from wordcloud import WordCloud 
wordcloud =WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

normal_words=" ".join([text for text in combi['tidy_tweet'][combi['label']==0]])
wordcloud =WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

negative_words=' '.join([text for text in combi['tidy_tweet'][combi['label']==1]])
wordcloud =WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

def hashtag_extract(x):
  hashtags =[]
  for i in x:
    ht= re.findall(r"#(\w+)", i)
    hashtags.append(ht)

  return hashtags

HT_regular =hashtag_extract(combi['tidy_tweet'][combi['label']==0])

HT_negative =hashtag_extract(combi['tidy_tweet'][combi['label']==1])

HT_regular =sum(HT_regular,[])
HT_negative =sum(HT_negative,[])

a=nltk.FreqDist(HT_regular)
d=pd.DataFrame({'Hashtag': list(a.keys()), 'Count': list(a.values())})
d= d.nlargest(columns='Count',n=10)
plt.figure(figsize=(16,5))
ax=sns.barplot(data=d,x='Hashtag',y='Count')
ax.set(ylabel='Count')
plt.show()

a=nltk.FreqDist(HT_negative)
d=pd.DataFrame({'Hashtag': list(a.keys()), 'Count': list(a.values())})
d= d.nlargest(columns='Count',n=10)
plt.figure(figsize=(16,5))
ax=sns.barplot(data=d,x='Hashtag',y='Count')
ax.set(ylabel='Count')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer =CountVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')

bow=bow_vectorizer.fit_transform(combi['tidy_tweet'])

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

train_bow=bow[:31962,:]
test_bow=bow[31962:,:]

xtrain_bow,xvalid_bow,ytrain,yvalid=train_test_split(train_bow,train['label'],random_state=42,test_size=0.3)

lreg=LogisticRegression()
lreg.fit(xtrain_bow,ytrain)

prediction=lreg.predict_proba(xvalid_bow)

prediction_int=prediction[:,1]>=0.3

prediction_int=prediction_int.astype(np.int)

f1_score(yvalid,prediction_int)

test_pred=lreg.predict_proba(test_bow)
test_pred_int=test_pred[:,1]>=0.3

test_pred_int=test_pred_int.astype(np.int)

test['label']=test_pred_int
submission=test[['id','label']]
submission.to_csv('sub_lreg_bow.csv',index=False)



